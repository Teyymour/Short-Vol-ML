{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "import random\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pandas_market_calendars import get_calendar\n",
    "import yfinance as yf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from databuilder import build_spread_backtest_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch base strategy backtest data + add relevant info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='/Users/teymour/Desktop/qnt-projs/Short-Vol-ML/.env')  # replace with your path\n",
    "polygon_api_key = os.getenv(\"POLYGON_API_KEY\")\n",
    "\n",
    "calendar = get_calendar(\"NYSE\")\n",
    "trading_dates = calendar.schedule(start_date=\"2023-04-20\", end_date=datetime.today()).index.strftime(\"%Y-%m-%d\").values\n",
    "\n",
    "#  Call the function from databuilder.py to generate a DataFrame with all relevant info for base strategy backtesting\n",
    "base_backtest_df = build_spread_backtest_dataset(dates=trading_dates, ticker='I:SPX', index_ticker=\"I:VIX1D\", \n",
    "                                              options_ticker=\"SPX\", trade_time=\"09:35\", move_adjustment=0.5, spread_width=1, polygon_api_key=polygon_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading assumptions and max loss calculations\n",
    "base_backtest_df['nat_price_cost'] = base_backtest_df['short_bid_price'] - base_backtest_df['long_ask_price']\n",
    "base_backtest_df['max_nat_price_loss'] = abs(base_backtest_df['short_strike'].iloc[0] - base_backtest_df['long_strike'].iloc[0]) - base_backtest_df['nat_price_cost']\n",
    "base_backtest_df['mid_price_cost'] = base_backtest_df['short_mid_price'] - base_backtest_df['long_mid_price']\n",
    "base_backtest_df['max_mid_price_loss'] = abs(base_backtest_df['short_strike'].iloc[0] - base_backtest_df['long_strike'].iloc[0]) - base_backtest_df['mid_price_cost']\n",
    "base_backtest_df[\"contracts\"] = 1\n",
    "base_backtest_df[\"fees\"] = base_backtest_df[\"contracts\"] * 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implied and realized volatility metrics\n",
    "base_backtest_df['trade_to_close_vol'] = abs((base_backtest_df['underlying_price_at_trade'] - base_backtest_df['underlying_closing_price']) / base_backtest_df['underlying_price_at_trade']) * 100\n",
    "base_backtest_df['current_day_IV'] = base_backtest_df['vix1d_value'] / np.sqrt(252)\n",
    "base_backtest_df['current_day_VRP'] = base_backtest_df['current_day_IV'] - base_backtest_df['trade_to_close_vol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pnl(row):\n",
    "    \"\"\"\n",
    "    Calculate the profit and loss (PnL) for the given row of backtest data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        A row of data containing information about the trade taken on a \n",
    "        given day\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The gross profit or loss (PnL) for the trade. If the calculated final PnL exceeds the maximum \n",
    "        allowable loss, it caps the loss at 'max_mid_price_loss'.\n",
    "    \"\"\"\n",
    "    if row['direction'] == 1:\n",
    "        settlement = row['underlying_closing_price'] - row['short_strike']\n",
    "        if settlement > 0:\n",
    "            settlement = 0\n",
    "            final_pnl = row['mid_price_cost']\n",
    "        else:\n",
    "            final_pnl = settlement + row['mid_price_cost']\n",
    "            \n",
    "    elif row['direction'] == 0:\n",
    "        settlement = row['short_strike'] - row['underlying_closing_price']\n",
    "        if settlement > 0:\n",
    "            settlement = 0\n",
    "            final_pnl = row['mid_price_cost']\n",
    "        else:\n",
    "            final_pnl = settlement + row['mid_price_cost']\n",
    "\n",
    "    gross_pnl = np.maximum(final_pnl, row['max_mid_price_loss'] * -1)\n",
    "    \n",
    "    return gross_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_backtest_df['gross_pnl'] = base_backtest_df.apply(calculate_pnl, axis=1)\n",
    "base_backtest_df['net_pnl'] = (base_backtest_df['gross_pnl'] * base_backtest_df['contracts']) - base_backtest_df['fees']\n",
    "\n",
    "# Assume we start with $3000\n",
    "capital = 3000\n",
    "\n",
    "base_backtest_df['net_capital'] = capital + (base_backtest_df['net_pnl']*100).cumsum()\n",
    "base_backtest_df['day_begin_net_capital'] = base_backtest_df['net_capital'] - (base_backtest_df['net_pnl']*100)\n",
    "base_backtest_df['pct_return'] = (base_backtest_df['net_pnl']* 100)/ base_backtest_df['day_begin_net_capital']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(returns, annualize=True, periods_per_year=252, risk_free_rate=0.0):\n",
    "    \"\"\"\n",
    "    Calculate the Sharpe ratio for a series of returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : pd.Series\n",
    "        A Pandas Series containing the returns, typically in percentage terms.\n",
    "    annualize : bool, optional (default=True)\n",
    "        Whether to annualize the Sharpe ratio.\n",
    "    periods_per_year : int, optional (default=252)\n",
    "        The number of trading periods in a year. Defaults to 252 (daily returns).\n",
    "    risk_free_rate : float, optional (default=0.0)\n",
    "        The risk-free rate of return. Defaults to 0 for simplicity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The calculated Sharpe ratio. If annualized, returns the annualized Sharpe ratio.\n",
    "    \"\"\"\n",
    "    excess_returns = returns - risk_free_rate / periods_per_year\n",
    "\n",
    "    mean_return = excess_returns.mean()\n",
    "    std_return = excess_returns.std()\n",
    "\n",
    "    sharpe_ratio = mean_return / std_return\n",
    "\n",
    "    if annualize:\n",
    "        sharpe_ratio *= np.sqrt(periods_per_year)\n",
    "    \n",
    "    return sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(base_backtest_df['net_capital']).show()\n",
    "print(f\"Base strategy Sharpe: {round(sharpe_ratio(returns=base_backtest_df['pct_return']), 2)}\")\n",
    "\n",
    "base_strat_win_rate = len(base_backtest_df[base_backtest_df['net_pnl'] > 0]) / len(base_backtest_df)\n",
    "base_strat_avg_win = base_backtest_df[base_backtest_df['net_pnl'] > 0]['net_pnl'].mean() * 100\n",
    "base_strat_avg_loss = abs(base_backtest_df[base_backtest_df['net_pnl'] < 0]['net_pnl'].mean() * 100)\n",
    "\n",
    "print(f\"Base strategy win rate: {round(base_strat_win_rate * 100, 2)}%\")\n",
    "print(f\"Base strategy average win: ${round(base_strat_avg_win, 2)}\")\n",
    "print(f\"Base strategy average loss: ${round(base_strat_avg_loss, 2)}\")\n",
    "print(f\"Base strategy expected value per trade: ${round((base_strat_avg_win * base_strat_win_rate) - (base_strat_avg_loss * (1 - base_strat_win_rate)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download historical OHLCV data for the S&P 500 to create features\n",
    "sp500_ticker = \"^GSPC\"\n",
    "underlying_feature_df = yf.download(sp500_ticker, start=\"2022-01-01\", end=\"2024-10-25\", interval=\"1d\")\n",
    "\n",
    "# preprocess the DataFrame\n",
    "underlying_feature_df.columns = underlying_feature_df.columns.get_level_values(0)\n",
    "underlying_feature_df.index = pd.to_datetime(underlying_feature_df.index).date\n",
    "underlying_feature_df = underlying_feature_df.rename_axis(\"t\", axis=\"index\")\n",
    "underlying_feature_df = underlying_feature_df.drop(columns=['Close'])\n",
    "underlying_feature_df.columns.name = None\n",
    "underlying_feature_df = underlying_feature_df.rename(columns={\n",
    "    'Open': 'o',\n",
    "    'High': 'h',\n",
    "    'Low': 'l',\n",
    "    'Adj Close': 'c',\n",
    "    'Volume': 'v'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features based on open price ('o') only\n",
    "for days in range(1, 6):\n",
    "    underlying_feature_df[f'return_{days}d'] = underlying_feature_df['o'].pct_change(periods=days)\n",
    "\n",
    "for lag in range(1, 6):\n",
    "    underlying_feature_df[f'lag_{lag}d'] = underlying_feature_df['o'].shift(lag)\n",
    "\n",
    "for lag in range(3, 6):\n",
    "    underlying_feature_df[f'serial_corr_{lag}d'] = underlying_feature_df['o'].rolling(window=lag).apply(lambda x: x.autocorr(), raw=False)\n",
    "\n",
    "ma_windows = [10, 50, 100, 200]\n",
    "for window in ma_windows:\n",
    "    underlying_feature_df[f'{window}d_MA'] = underlying_feature_df['o'].rolling(window=window).mean()\n",
    "\n",
    "underlying_feature_df['MA_crossover_10_50'] = np.where(underlying_feature_df['10d_MA'] > underlying_feature_df['50d_MA'], 1, 0)\n",
    "\n",
    "underlying_feature_df['price_to_50d_MA'] = underlying_feature_df['o'] / underlying_feature_df['50d_MA']\n",
    "underlying_feature_df['price_to_200d_MA'] = underlying_feature_df['o'] / underlying_feature_df['200d_MA']\n",
    "\n",
    "underlying_feature_df['12d_EMA'] = underlying_feature_df['o'].ewm(span=12, adjust=False).mean()\n",
    "underlying_feature_df['26d_EMA'] = underlying_feature_df['o'].ewm(span=26, adjust=False).mean()\n",
    "underlying_feature_df['MACD'] = underlying_feature_df['12d_EMA'] - underlying_feature_df['26d_EMA']\n",
    "underlying_feature_df['MACD_signal'] = underlying_feature_df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "underlying_feature_df['momentum_5d'] = underlying_feature_df['o'] / underlying_feature_df['o'].shift(5) - 1\n",
    "underlying_feature_df['momentum_10d'] = underlying_feature_df['o'] / underlying_feature_df['o'].shift(10) - 1\n",
    "\n",
    "underlying_feature_df['20d_MA'] = underlying_feature_df['o'].rolling(window=20).mean()\n",
    "underlying_feature_df['20d_stddev'] = underlying_feature_df['o'].rolling(window=20).std()\n",
    "underlying_feature_df['upper_band'] = underlying_feature_df['20d_MA'] + (underlying_feature_df['20d_stddev'] * 2)\n",
    "underlying_feature_df['lower_band'] = underlying_feature_df['20d_MA'] - (underlying_feature_df['20d_stddev'] * 2)\n",
    "\n",
    "vol_windows = [10, 20, 50, 100]\n",
    "for window in vol_windows:\n",
    "    underlying_feature_df[f'{window}d_volatility'] = underlying_feature_df['o'].rolling(window=window).std()\n",
    "\n",
    "underlying_feature_df['volatility_ratio'] = underlying_feature_df['10d_volatility'] / underlying_feature_df['50d_volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that require end-of-day information (close, high, low)\n",
    "underlying_feature_df['high_low_range'] = underlying_feature_df['h'] - underlying_feature_df['l']\n",
    "for window in vol_windows:\n",
    "    underlying_feature_df[f'{window}d_high_low_vol'] = underlying_feature_df['high_low_range'].rolling(window=window).std()\n",
    "\n",
    "underlying_feature_df['tr'] = np.maximum(\n",
    "    (underlying_feature_df['h'] - underlying_feature_df['l']),\n",
    "    np.maximum(\n",
    "        abs(underlying_feature_df['h'] - underlying_feature_df['c'].shift(1)),\n",
    "        abs(underlying_feature_df['l'] - underlying_feature_df['c'].shift(1))\n",
    "    )\n",
    ")\n",
    "underlying_feature_df['14d_ATR'] = underlying_feature_df['tr'].rolling(window=14).mean()\n",
    "\n",
    "underlying_feature_df['20d_high'] = underlying_feature_df['h'].rolling(window=20).max()\n",
    "underlying_feature_df['20d_low'] = underlying_feature_df['l'].rolling(window=20).min()\n",
    "\n",
    "underlying_feature_df['14d_ATRP'] = underlying_feature_df['14d_ATR'] / underlying_feature_df['c'] * 100\n",
    "\n",
    "def compute_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "underlying_feature_df['14d_RSI'] = compute_rsi(underlying_feature_df['c'], window=14)\n",
    "\n",
    "# Drop intermediate columns used for calculations\n",
    "underlying_feature_df.drop(['tr', '20d_stddev', '20d_MA'], axis=1, inplace=True)\n",
    "\n",
    "lagged_feature_list = [\n",
    "    'high_low_range', '10d_high_low_vol', '20d_high_low_vol', '50d_high_low_vol', '100d_high_low_vol',\n",
    "    '14d_ATR', '20d_high', '20d_low', '14d_ATRP', '14d_RSI'\n",
    "]\n",
    "\n",
    "for f in lagged_feature_list:\n",
    "    underlying_feature_df[f] = underlying_feature_df[f].shift()\n",
    "    underlying_feature_df = underlying_feature_df.rename(columns={f: f'prev_{f}'})\n",
    "\n",
    "underlying_feature_df = underlying_feature_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable - whether we should have traded on a given day or not\n",
    "base_backtest_df['avoid_trade'] = np.where(base_backtest_df['net_pnl'] < 0, 1, 0 )\n",
    "\n",
    "# add the correct targets to the feature DataFrame\n",
    "aligned_target = base_backtest_df['avoid_trade'].reindex(underlying_feature_df.index)\n",
    "underlying_feature_df = pd.concat([underlying_feature_df, aligned_target], axis=1)\n",
    "underlying_feature_df = underlying_feature_df.rename(columns={'avoid_trade': 'target'})\n",
    "\n",
    "# Store the features as a variable for quick access\n",
    "X = underlying_feature_df.drop(['o', 'c', 'h', 'l', 'target'], axis=1)\n",
    "\n",
    "# Drop null target values and null feature values that were created by lags)\n",
    "underlying_feature_df = underlying_feature_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_period(df, training_period_length, backtest_period_length):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into sequential training and backtesting periods for walk-forward training and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the time-series data to be split into training and backtesting periods.\n",
    "    training_period_length : int\n",
    "        The number of data points (e.g., days) to include in each training period.\n",
    "    backtest_period_length : int\n",
    "        The number of data points to include in each backtesting period.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - training_keys (list of tuples): A list of (start_index, end_index) pairs for each training period.\n",
    "        - backtest_keys (list of tuples): A list of (start_index, end_index) pairs for each backtesting period.\n",
    "        - training_period_data_dict (dict): A dictionary where each key is a (start_index, end_index) pair \n",
    "          representing a training period, and each value is a DataFrame containing the data for that period.\n",
    "        - backtest_period_data_dict (dict): A dictionary where each key is a (start_index, end_index) pair \n",
    "          representing a backtesting period, and each value is a DataFrame containing the data for that period.\n",
    "    \"\"\"\n",
    "    training_keys = []\n",
    "    backtest_keys = []\n",
    "    training_period_data_dict = {}\n",
    "    backtest_period_data_dict = {}\n",
    "\n",
    "    current_end_index = len(df)\n",
    "\n",
    "    while current_end_index - backtest_period_length - training_period_length >= 0:\n",
    "        backtest_end_index = current_end_index\n",
    "        backtest_start_index = backtest_end_index - backtest_period_length\n",
    "        training_end_index = backtest_start_index\n",
    "        training_start_index = training_end_index - training_period_length\n",
    "\n",
    "        if training_start_index < 0:\n",
    "            break\n",
    "\n",
    "        training_df = df.iloc[training_start_index:training_end_index]\n",
    "        backtest_df = df.iloc[backtest_start_index:backtest_end_index]\n",
    "\n",
    "        training_keys.append((training_start_index, training_end_index))\n",
    "        backtest_keys.append((backtest_start_index, backtest_end_index))\n",
    "\n",
    "        training_period_data_dict[(training_start_index, training_end_index)] = training_df\n",
    "        backtest_period_data_dict[(backtest_start_index, backtest_end_index)] = backtest_df\n",
    "\n",
    "        current_end_index = backtest_start_index\n",
    "\n",
    "    training_keys.reverse()\n",
    "    backtest_keys.reverse()\n",
    "    training_period_data_dict = {k: training_period_data_dict[k] for k in reversed(training_period_data_dict)}\n",
    "    backtest_period_data_dict = {k: backtest_period_data_dict[k] for k in reversed(backtest_period_data_dict)}\n",
    "    \n",
    "    return training_keys, backtest_keys, training_period_data_dict, backtest_period_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purged_k_fold_generator(data, n_splits, embargo_pct):\n",
    "    \"\"\"\n",
    "    Generates purged K-fold cross-validation splits for time series data as a list of tuples,\n",
    "    ensuring no data leakage between training and test sets by adding an embargo period.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataset to split into training and test sets for each fold, indexed by date.\n",
    "    n_splits : int\n",
    "        The number of folds for cross-validation.\n",
    "    embargo_pct : float\n",
    "        The percentage of data to \"embargo\" between training and test sets to prevent data leakage. \n",
    "        Typically, a small value like 0.01 (1%) is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    folds : list of tuples\n",
    "        A list of (train_data, test_data) tuples for each fold. Each tuple contains:\n",
    "        - train_data : pd.DataFrame\n",
    "            The training set for the current fold.\n",
    "        - test_data : pd.DataFrame\n",
    "            The test set for the current fold, with an embargo period before it.\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    fold_size = len(data) // n_splits\n",
    "    for i in range(n_splits):\n",
    "        train_end = fold_size * i\n",
    "        test_start = train_end + int(fold_size * embargo_pct)\n",
    "        test_end = fold_size * (i + 1)\n",
    "\n",
    "        train_data = data.iloc[:train_end]\n",
    "        test_data = data.iloc[test_start:test_end]\n",
    "\n",
    "        if not train_data.empty and not test_data.empty:\n",
    "            folds.append((train_data, test_data))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def random_search_with_purged_kfold(data, feature_list, param_grid, max_iter=10, n_splits=3, embargo_pct=0.01, target_column='target'):\n",
    "    \"\"\"\n",
    "    Performs random search for hyperparameter tuning using purged K-fold cross-validation on a \n",
    "    CatBoostClassifier. The best parameters are selected based on the highest F1 score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataset containing features and target labels, indexed by date.\n",
    "    feature_list : list of str\n",
    "        List of feature names to use in the model.\n",
    "    param_grid : dict\n",
    "        Dictionary specifying the parameter grid for random search. Keys are parameter names and values are lists \n",
    "        of parameter values to sample from.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of parameter combinations to sample for random search. Default is 10.\n",
    "    n_splits : int, optional\n",
    "        Number of folds for cross-validation. Default is 3.\n",
    "    embargo_pct : float, optional\n",
    "        Percentage of data to embargo between training and test sets for each fold. Default is 0.01 (1%).\n",
    "    target_column : str, optional\n",
    "        The name of the target column in the data. Default is 'target'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_params : dict\n",
    "        Dictionary of the best-performing hyperparameters.\n",
    "    best_score : float\n",
    "        The highest average F1 score achieved during cross-validation.\n",
    "    \"\"\"\n",
    "    best_params = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    # Generate all parameter combinations, then randomly sample `max_iter` combinations\n",
    "    all_params = list(ParameterGrid(param_grid))\n",
    "    sampled_params = random.sample(all_params, min(max_iter, len(all_params)))\n",
    "\n",
    "    folds = purged_k_fold_generator(data, n_splits, embargo_pct)\n",
    "\n",
    "    for params in tqdm(sampled_params, desc=\"Random Search Progress\"):\n",
    "        scores = []\n",
    "\n",
    "        for train_df, val_df in folds:\n",
    "            scaler = StandardScaler()\n",
    "            train_df.loc[:, feature_list] = scaler.fit_transform(train_df[feature_list]).astype('float64')\n",
    "            val_df.loc[:, feature_list] = scaler.transform(val_df[feature_list]).astype('float64')\n",
    "\n",
    "            X_train, y_train = train_df[feature_list], train_df[target_column].values.flatten()\n",
    "            X_val, y_val = val_df[feature_list], val_df[target_column].values.flatten()\n",
    "\n",
    "            model = CatBoostClassifier(\n",
    "                loss_function='Logloss',\n",
    "                class_weights=params.get('class_weights', None),\n",
    "                depth=params.get('depth', 6),\n",
    "                learning_rate=params.get('learning_rate', 0.1),\n",
    "                thread_count=-1,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=(X_val, y_val),\n",
    "                early_stopping_rounds=50,\n",
    "                use_best_model=True\n",
    "            )\n",
    "\n",
    "            y_pred = (model.predict_proba(X_val)[:, 1] > params.get('prediction_threshold', 0.5)).astype(int)\n",
    "            score = f1_score(y_val, y_pred)\n",
    "            scores.append(score)\n",
    "\n",
    "        avg_score = np.mean(scores)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metalabel(data, training_periods, testing_periods, quant_feature_list, cat_feature_list, param_grid):\n",
    "\n",
    "    data = data[:-1].copy()\n",
    "    \n",
    "    best_params, best_score = random_search_with_purged_kfold(data, quant_feature_list, param_grid, max_iter=500)\n",
    "\n",
    "    print(f\"Best parameters found: {best_params} with F1 score: {best_score}\")\n",
    "\n",
    "    keys, backtest_keys, period_data_dict, backtest_period_data_dict = group_by_period(\n",
    "        data, training_periods, testing_periods\n",
    "    )\n",
    "\n",
    "    agg_backtest_df = pd.DataFrame()\n",
    "    num_iterations = len(keys)\n",
    "\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        model_key = keys[i]\n",
    "        train_df = period_data_dict[model_key].copy()\n",
    "        scaler = StandardScaler()\n",
    "        train_df[quant_feature_list] = scaler.fit_transform(train_df[quant_feature_list])\n",
    "\n",
    "        all_features = quant_feature_list + cat_feature_list\n",
    "        split_idx = int(len(train_df) * 0.8)\n",
    "\n",
    "        X_train = train_df[all_features].iloc[:split_idx]\n",
    "        y_train = train_df['target'].iloc[:split_idx].values.flatten()\n",
    "        X_val = train_df[all_features].iloc[split_idx:]\n",
    "        y_val = train_df['target'].iloc[split_idx:].values.flatten()\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='F1',\n",
    "            class_weights=best_params['class_weights'],\n",
    "            depth=best_params['depth'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            thread_count=-1\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True,\n",
    "            plot=False,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        backtest_key = backtest_keys[i]\n",
    "        backtest_df = backtest_period_data_dict[backtest_key].copy()\n",
    "        \n",
    "        backtest_df[quant_feature_list] = scaler.transform(backtest_df[quant_feature_list])\n",
    "        test_features = backtest_df[all_features]\n",
    "\n",
    "        probabilities = model.predict_proba(test_features)[:, 1]\n",
    "        predictions = (probabilities > best_params['prediction_threshold']).astype(int)\n",
    "        confidence = np.maximum(probabilities, 1 - probabilities)\n",
    "\n",
    "        prediction_df = pd.DataFrame({\n",
    "            'predicted_avoid_trade': predictions,\n",
    "            'prediction_confidence': confidence,\n",
    "            'raw_probability': probabilities\n",
    "        }, index=backtest_df.index)\n",
    "\n",
    "        backtest_df = backtest_df.join(prediction_df)\n",
    "\n",
    "        agg_backtest_df = pd.concat([agg_backtest_df, backtest_df], axis=0)\n",
    "\n",
    "    return agg_backtest_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e732d323204249f187b9526a5a67625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Random Search Progress:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'iterations': [200, 500, 1000],  # Number of trees\n",
    "    'class_weights': [[1, 3], [1, 3.5], [1, 4], [1, 4.5], [1, 5]],  # Adjust class imbalance\n",
    "    'depth': [3, 4, 6, 8, 10],  # Depth of trees\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.2],  # Step size shrinkage\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],  # L2 regularization coefficient\n",
    "    'border_count': [32, 64, 128, 254],  # Number of splits for numerical features\n",
    "    'bagging_temperature': [0, 0.5, 1, 2, 5],  # Used when sampling_type='Bayesian' for sampling temperature\n",
    "    'random_strength': [1, 2, 5, 10],  # Amount of randomness in score calculation\n",
    "    'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS'],  # Type of bootstrapping\n",
    "    'subsample': [0.5, 0.8, 1],  # Subsample ratio for bagging (only used with 'Bernoulli' or 'MVS' bootstrap types)\n",
    "    'one_hot_max_size': [2, 5, 10, 20],  # Maximum number of unique values for categorical features to use one-hot encoding\n",
    "}\n",
    "\n",
    "metalabeled_backtest_df = metalabel(\n",
    "    data=underlying_feature_df, \n",
    "    training_periods=150, \n",
    "    testing_periods=1, \n",
    "    quant_feature_list=list(X.columns), \n",
    "    cat_feature_list=[],\n",
    "    param_grid=param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = metalabeled_backtest_df['target']\n",
    "y_pred = metalabeled_backtest_df['predicted_avoid_trade']\n",
    "\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics and display results for a binary classification model.\n",
    "\n",
    "    This function calculates the accuracy, precision, recall, and F1 score for a given set of \n",
    "    true and predicted labels. It also prints the confusion matrix and a detailed classification \n",
    "    report with precision, recall, F1 score, and support for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like or pd.Series\n",
    "        The ground truth (actual) labels.\n",
    "    y_pred : array-like or pd.Series\n",
    "        The predicted labels by the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        A detailed classification report is printed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Detailed classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "evaluate_classification(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalabeled_backtest_df[(metalabeled_backtest_df['target'] == 1) & (metalabeled_backtest_df['predicted_avoid_trade'] == 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the meta-model's trade/no trade reccomendations and add them to the base strategy backtest DataFrame to create a meta-labeled backtest DataFrame.\n",
    "# We will now only calculate PnL for a given day if the meta-model recommends to trade, setting net PnL to 0, effectively skipping the day otherwise\n",
    "metalabeled_strat_backtest_df = base_backtest_df.copy()\n",
    "aligned_preds = metalabeled_backtest_df['predicted_avoid_trade'].reindex(metalabeled_strat_backtest_df.index)\n",
    "metalabeled_strat_backtest_df['predicted_avoid_trade'] = aligned_preds\n",
    "metalabeled_strat_backtest_df = metalabeled_strat_backtest_df.dropna()\n",
    "metalabeled_strat_backtest_df['predicted_avoid_trade'] = metalabeled_strat_backtest_df['predicted_avoid_trade'].astype(int)\n",
    "\n",
    "metalabeled_strat_backtest_df['gross_pnl'] = metalabeled_strat_backtest_df.apply(calculate_pnl, axis=1)\n",
    "metalabeled_strat_backtest_df['net_pnl'] = np.where(metalabeled_strat_backtest_df['predicted_avoid_trade'] == 0, metalabeled_strat_backtest_df['gross_pnl'] * metalabeled_strat_backtest_df['contracts'] - metalabeled_strat_backtest_df['fees'], 0)\n",
    "\n",
    "capital = 3000\n",
    "\n",
    "metalabeled_strat_backtest_df['net_capital'] = capital + (metalabeled_strat_backtest_df['net_pnl']*100).cumsum()\n",
    "metalabeled_strat_backtest_df['day_begin_net_capital'] = metalabeled_strat_backtest_df['net_capital'] - (metalabeled_strat_backtest_df['net_pnl']*100)\n",
    "metalabeled_strat_backtest_df['cumulative_pnl'] = metalabeled_strat_backtest_df['net_pnl'].cumsum()\n",
    "metalabeled_strat_backtest_df['pct_return'] = (metalabeled_strat_backtest_df['net_pnl']* 100)/ metalabeled_strat_backtest_df['day_begin_net_capital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(metalabeled_strat_backtest_df['net_capital']).show()\n",
    "print(f\"Meta-labeled strategy Sharpe: {sharpe_ratio(returns=metalabeled_strat_backtest_df['pct_return'])}\")\n",
    "\n",
    "metalabeled_strat_win_rate = len(metalabeled_strat_backtest_df[metalabeled_strat_backtest_df['net_pnl'] > 0]) / len(metalabeled_strat_backtest_df[metalabeled_strat_backtest_df['predicted_avoid_trade'] == 0])\n",
    "metalabeled_strat_avg_win = metalabeled_strat_backtest_df[metalabeled_strat_backtest_df['net_pnl'] > 0]['net_pnl'].mean() * 100\n",
    "metalabeled_strat_avg_loss = abs(metalabeled_strat_backtest_df[metalabeled_strat_backtest_df['net_pnl'] < 0]['net_pnl'].mean() * 100)\n",
    "\n",
    "print(f\"Meta-labeled strategy win rate: {round(metalabeled_strat_win_rate * 100, 2)}%\")\n",
    "print(f\"Meta-labeled strategy average win: ${round(metalabeled_strat_avg_win, 2)}\")\n",
    "print(f\"Meta-labeled strategy average loss: ${round(metalabeled_strat_avg_loss, 2)}\")\n",
    "print(f\"Meta-labeled strategy expected value per trade: ${round((metalabeled_strat_avg_win * metalabeled_strat_win_rate) - (metalabeled_strat_avg_loss * (1 - metalabeled_strat_win_rate)), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalabeled_strat_backtest_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
