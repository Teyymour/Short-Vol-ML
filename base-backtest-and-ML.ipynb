{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "import random\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pandas_market_calendars import get_calendar\n",
    "import yfinance as yf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from databuilder import build_spread_backtest_dataset\n",
    "\n",
    "def seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for multiple libraries to ensure reproducibility.\n",
    "    \n",
    "    Parameters:\n",
    "    seed (int): The seed value to set across libraries.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Base Strategy Backtest Data + Add Relevant Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='Short-Vol-ML/.env')  # replace with your path\n",
    "polygon_api_key = os.getenv(\"POLYGON_API_KEY\")\n",
    "\n",
    "calendar = get_calendar(\"NYSE\")\n",
    "trading_dates = calendar.schedule(start_date=\"2023-04-20\", end_date=\"2024-10-24\").index.strftime(\"%Y-%m-%d\").values\n",
    "\n",
    "#  Call the function from databuilder.py to generate a DataFrame with all relevant info for base strategy backtesting\n",
    "base_backtest_df = build_spread_backtest_dataset(dates=trading_dates, ticker='I:SPX', index_ticker=\"I:VIX1D\", \n",
    "                                              options_ticker=\"SPX\", trade_time=\"09:35\", move_adjustment=0.5, spread_width=1, polygon_api_key=polygon_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading assumptions and max loss calculations\n",
    "base_backtest_df['nat_price_cost'] = base_backtest_df['short_bid_price'] - base_backtest_df['long_ask_price']\n",
    "base_backtest_df['max_nat_price_loss'] = abs(base_backtest_df['short_strike'].iloc[0] - base_backtest_df['long_strike'].iloc[0]) - base_backtest_df['nat_price_cost']\n",
    "base_backtest_df['mid_price_cost'] = base_backtest_df['short_mid_price'] - base_backtest_df['long_mid_price']\n",
    "base_backtest_df['max_mid_price_loss'] = abs(base_backtest_df['short_strike'].iloc[0] - base_backtest_df['long_strike'].iloc[0]) - base_backtest_df['mid_price_cost']\n",
    "base_backtest_df[\"contracts\"] = 1\n",
    "base_backtest_df[\"fees\"] = base_backtest_df[\"contracts\"] * 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pnl(row):\n",
    "    \"\"\"\n",
    "    Calculate the profit and loss (PnL) for the given row of backtest data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        A row of data containing information about the trade taken on a \n",
    "        given day\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The gross profit or loss (PnL) for the trade. If the calculated final PnL exceeds the maximum \n",
    "        allowable loss, it caps the loss at 'max_mid_price_loss'.\n",
    "    \"\"\"\n",
    "    if row['direction'] == 1:\n",
    "        settlement = row['underlying_closing_price'] - row['short_strike']\n",
    "        if settlement > 0:\n",
    "            settlement = 0\n",
    "            final_pnl = row['mid_price_cost']\n",
    "        else:\n",
    "            final_pnl = settlement + row['mid_price_cost']\n",
    "            \n",
    "    elif row['direction'] == 0:\n",
    "        settlement = row['short_strike'] - row['underlying_closing_price']\n",
    "        if settlement > 0:\n",
    "            settlement = 0\n",
    "            final_pnl = row['mid_price_cost']\n",
    "        else:\n",
    "            final_pnl = settlement + row['mid_price_cost']\n",
    "\n",
    "    gross_pnl = np.maximum(final_pnl, row['max_mid_price_loss'] * -1)\n",
    "    \n",
    "    return gross_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info that will be used in determining meta-model targets and in future base strat backtest\n",
    "base_backtest_df['gross_pnl'] = base_backtest_df.apply(calculate_pnl, axis=1)\n",
    "base_backtest_df['net_pnl'] = base_backtest_df['gross_pnl'] * base_backtest_df['contracts'] - base_backtest_df['fees']\n",
    "\n",
    "capital = 3000\n",
    "\n",
    "base_backtest_df['net_capital'] = capital + (base_backtest_df['net_pnl']*100).cumsum()\n",
    "base_backtest_df['day_begin_net_capital'] = base_backtest_df['net_capital'] - (base_backtest_df['net_pnl']*100)\n",
    "base_backtest_df['cumulative_pnl'] = base_backtest_df['net_pnl'].cumsum()\n",
    "base_backtest_df['pct_return'] = (base_backtest_df['net_pnl']* 100)/ base_backtest_df['day_begin_net_capital']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download historical OHLCV data for the S&P 500 to create features\n",
    "sp500_ticker = \"^GSPC\"\n",
    "underlying_feature_df = yf.download(sp500_ticker, start=\"2022-01-01\", end=\"2024-10-25\", interval=\"1d\")\n",
    "\n",
    "# preprocess the DataFrame\n",
    "underlying_feature_df.columns = underlying_feature_df.columns.get_level_values(0)\n",
    "underlying_feature_df.index = pd.to_datetime(underlying_feature_df.index).date\n",
    "underlying_feature_df = underlying_feature_df.rename_axis(\"t\", axis=\"index\")\n",
    "underlying_feature_df = underlying_feature_df.drop(columns=['Close'])\n",
    "underlying_feature_df.columns.name = None\n",
    "underlying_feature_df = underlying_feature_df.rename(columns={\n",
    "    'Open': 'o',\n",
    "    'High': 'h',\n",
    "    'Low': 'l',\n",
    "    'Adj Close': 'c',\n",
    "    'Volume': 'v'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-lagged features using opening price\n",
    "for window in [5, 10, 20, 50]:\n",
    "   underlying_feature_df[f'{window}d_volatility'] = underlying_feature_df['o'].rolling(window=window).std()\n",
    "\n",
    "underlying_feature_df['vol_of_vol_10d'] = underlying_feature_df['10d_volatility'].rolling(window=10).std()\n",
    "underlying_feature_df['volatility_ratio'] = underlying_feature_df['10d_volatility'] / underlying_feature_df['50d_volatility']\n",
    "underlying_feature_df['volatility_ratio_5_20'] = underlying_feature_df['5d_volatility'] / underlying_feature_df['20d_volatility']\n",
    "\n",
    "for window in [20, 50, 200]:\n",
    "   underlying_feature_df[f'{window}d_MA'] = underlying_feature_df['o'].rolling(window=window).mean()\n",
    "   underlying_feature_df[f'price_to_{window}d_MA'] = underlying_feature_df['o'] / underlying_feature_df[f'{window}d_MA']\n",
    "\n",
    "for days in [3, 5, 10]:\n",
    "   underlying_feature_df[f'abs_return_{days}d'] = abs(underlying_feature_df['o'].pct_change(periods=days))\n",
    "\n",
    "underlying_feature_df['overnight_gap'] = underlying_feature_df['o'] / underlying_feature_df['o'].shift(1) - 1\n",
    "underlying_feature_df['avg_gap_5d'] = underlying_feature_df['overnight_gap'].rolling(window=5).mean().abs()\n",
    "underlying_feature_df['consecutive_gaps'] = (\n",
    "   (underlying_feature_df['overnight_gap'].abs() > \n",
    "    underlying_feature_df['overnight_gap'].rolling(window=20).std())\n",
    "   .rolling(window=3).sum()\n",
    ")\n",
    "\n",
    "underlying_feature_df['volatility_trend'] = (\n",
    "   underlying_feature_df['5d_volatility'] / \n",
    "   underlying_feature_df['5d_volatility'].rolling(window=10).mean()\n",
    ")\n",
    "\n",
    "def compute_rsi(data, window=14):\n",
    "   delta = data.diff()\n",
    "   gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "   loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "   rs = gain / loss\n",
    "   rsi = 100 - (100 / (1 + rs))\n",
    "   return rsi\n",
    "\n",
    "underlying_feature_df['14d_RSI'] = compute_rsi(underlying_feature_df['o'], window=14)\n",
    "\n",
    "# Features that need to be lagged (using previous day's data)\n",
    "underlying_feature_df['high_low_range'] = underlying_feature_df['h'] - underlying_feature_df['l']\n",
    "\n",
    "underlying_feature_df['tr'] = np.maximum(\n",
    "   (underlying_feature_df['h'] - underlying_feature_df['l']),\n",
    "   np.maximum(\n",
    "       abs(underlying_feature_df['h'] - underlying_feature_df['c'].shift(1)),\n",
    "       abs(underlying_feature_df['l'] - underlying_feature_df['c'].shift(1))\n",
    "   )\n",
    ")\n",
    "underlying_feature_df['14d_ATR'] = underlying_feature_df['tr'].rolling(window=14).mean()\n",
    "underlying_feature_df['14d_ATRP'] = underlying_feature_df['14d_ATR'] / underlying_feature_df['c'] * 100\n",
    "\n",
    "lagged_feature_list = [\n",
    "   'high_low_range', '14d_ATR', '14d_ATRP'\n",
    "]\n",
    "\n",
    "for f in lagged_feature_list:\n",
    "   underlying_feature_df[f] = underlying_feature_df[f].shift()\n",
    "   underlying_feature_df = underlying_feature_df.rename(columns={f: f'prev_{f}'})\n",
    "\n",
    "underlying_feature_df = underlying_feature_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable - whether we should have traded on a given day or not\n",
    "base_backtest_df['avoid_trade'] = np.where(base_backtest_df['net_pnl'] < 0, 1, 0 )\n",
    "\n",
    "# add the correct targets to the feature DataFrame\n",
    "aligned_target = base_backtest_df['avoid_trade'].reindex(underlying_feature_df.index)\n",
    "underlying_feature_df = pd.concat([underlying_feature_df, aligned_target], axis=1)\n",
    "underlying_feature_df = underlying_feature_df.rename(columns={'avoid_trade': 'target'})\n",
    "\n",
    "# Store the features as a variable for quick access\n",
    "X = underlying_feature_df.drop(['o', 'c', 'h', 'l', 'target'], axis=1)\n",
    "\n",
    "# Drop null target values and null feature values that were created by lags)\n",
    "underlying_feature_df = underlying_feature_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_period(df, training_period_length, backtest_period_length):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into sequential training and backtesting periods for walk-forward training and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the time-series data to be split into training and backtesting periods.\n",
    "    training_period_length : int\n",
    "        The number of data points (e.g., days) to include in each training period.\n",
    "    backtest_period_length : int\n",
    "        The number of data points to include in each backtesting period.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - training_keys (list of tuples): A list of (start_index, end_index) pairs for each training period.\n",
    "        - backtest_keys (list of tuples): A list of (start_index, end_index) pairs for each backtesting period.\n",
    "        - training_period_data_dict (dict): A dictionary where each key is a (start_index, end_index) pair \n",
    "          representing a training period, and each value is a DataFrame containing the data for that period.\n",
    "        - backtest_period_data_dict (dict): A dictionary where each key is a (start_index, end_index) pair \n",
    "          representing a backtesting period, and each value is a DataFrame containing the data for that period.\n",
    "    \"\"\"\n",
    "    training_keys = []\n",
    "    backtest_keys = []\n",
    "    training_period_data_dict = {}\n",
    "    backtest_period_data_dict = {}\n",
    "\n",
    "    current_end_index = len(df)\n",
    "\n",
    "    while current_end_index - backtest_period_length - training_period_length >= 0:\n",
    "        backtest_end_index = current_end_index\n",
    "        backtest_start_index = backtest_end_index - backtest_period_length\n",
    "        training_end_index = backtest_start_index\n",
    "        training_start_index = training_end_index - training_period_length\n",
    "\n",
    "        if training_start_index < 0:\n",
    "            break\n",
    "\n",
    "        training_df = df.iloc[training_start_index:training_end_index]\n",
    "        backtest_df = df.iloc[backtest_start_index:backtest_end_index]\n",
    "\n",
    "        training_keys.append((training_start_index, training_end_index))\n",
    "        backtest_keys.append((backtest_start_index, backtest_end_index))\n",
    "\n",
    "        training_period_data_dict[(training_start_index, training_end_index)] = training_df\n",
    "        backtest_period_data_dict[(backtest_start_index, backtest_end_index)] = backtest_df\n",
    "\n",
    "        current_end_index = backtest_start_index\n",
    "\n",
    "    training_keys.reverse()\n",
    "    backtest_keys.reverse()\n",
    "    training_period_data_dict = {k: training_period_data_dict[k] for k in reversed(training_period_data_dict)}\n",
    "    backtest_period_data_dict = {k: backtest_period_data_dict[k] for k in reversed(backtest_period_data_dict)}\n",
    "    \n",
    "    return training_keys, backtest_keys, training_period_data_dict, backtest_period_data_dict\n",
    "\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics and display results for a binary classification model.\n",
    "\n",
    "    This function calculates the accuracy, precision, recall, and F1 score for a given set of \n",
    "    true and predicted labels. It also prints the confusion matrix and a detailed classification \n",
    "    report with precision, recall, F1 score, and support for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like or pd.Series\n",
    "        The ground truth (actual) labels.\n",
    "    y_pred : array-like or pd.Series\n",
    "        The predicted labels by the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        A detailed classification report is printed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Detailed classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metalabel(data, training_periods, testing_periods, quant_feature_list, cat_feature_list):\n",
    "    \"\"\"\n",
    "    Performs metalabeling on time-series data using a walk-forward approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The time-series dataset with features and a target label, indexed by date.\n",
    "    training_periods : int\n",
    "        The number of data points (e.g., days) in each training period for walk-forward validation.\n",
    "    testing_periods : int\n",
    "        The number of data points in each testing period.\n",
    "    quant_feature_list : list of str\n",
    "        List of quantitative feature names to use in the model.\n",
    "    cat_feature_list : list of str\n",
    "        List of categorical feature names to use in the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    agg_backtest_df : pd.DataFrame\n",
    "        A DataFrame containing backtesting data with model predictions and their confidences.\n",
    "    \"\"\"\n",
    "    data = data[:-1].copy()\n",
    "    \n",
    "    # best_params, best_score = random_search_with_purged_kfold(data, quant_feature_list, param_grid, max_iter=100)\n",
    "\n",
    "    # print(f\"Best parameters found: {best_params} with F1 score: {best_score}\")\n",
    "\n",
    "    keys, backtest_keys, period_data_dict, backtest_period_data_dict = group_by_period(\n",
    "        data, training_periods, testing_periods\n",
    "    )\n",
    "\n",
    "    agg_backtest_df = pd.DataFrame()\n",
    "    num_iterations = len(keys)\n",
    "\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        model_key = keys[i]\n",
    "        train_df = period_data_dict[model_key].copy()\n",
    "        scaler = StandardScaler()\n",
    "        train_df[quant_feature_list] = scaler.fit_transform(train_df[quant_feature_list])\n",
    "\n",
    "        all_features = quant_feature_list + cat_feature_list\n",
    "        split_idx = int(len(train_df) * 0.8)\n",
    "\n",
    "        X_train = train_df[all_features].iloc[:split_idx]\n",
    "        y_train = train_df['target'].iloc[:split_idx].values.flatten()\n",
    "        X_val = train_df[all_features].iloc[split_idx:]\n",
    "        y_val = train_df['target'].iloc[split_idx:].values.flatten()\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='F1',\n",
    "            thread_count=-1,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True,\n",
    "            plot=False,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        backtest_key = backtest_keys[i]\n",
    "        backtest_df = backtest_period_data_dict[backtest_key].copy()\n",
    "        \n",
    "        backtest_df[quant_feature_list] = scaler.transform(backtest_df[quant_feature_list])\n",
    "        test_features = backtest_df[all_features]\n",
    "\n",
    "        probabilities = model.predict_proba(test_features)[:, 1]\n",
    "        predictions = (probabilities > .5).astype(int)\n",
    "        confidence = np.maximum(probabilities, 1 - probabilities)\n",
    "\n",
    "        prediction_df = pd.DataFrame({\n",
    "            'predicted_avoid_trade': predictions,\n",
    "            'prediction_confidence': confidence,\n",
    "            'raw_probability': probabilities\n",
    "        }, index=backtest_df.index)\n",
    "\n",
    "        backtest_df = backtest_df.join(prediction_df)\n",
    "\n",
    "        agg_backtest_df = pd.concat([agg_backtest_df, backtest_df], axis=0)\n",
    "\n",
    "    return agg_backtest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalabeled_backtest_df = metalabel(\n",
    "    data=underlying_feature_df, \n",
    "    training_periods=140,\n",
    "    testing_periods=1, \n",
    "    quant_feature_list=list(X.columns), \n",
    "    cat_feature_list=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = metalabeled_backtest_df['target']\n",
    "y_pred = metalabeled_backtest_df['predicted_avoid_trade']\n",
    "evaluate_classification(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Backtests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(returns, annualize=True, periods_per_year=252, risk_free_rate=0.04):\n",
    "    \"\"\"\n",
    "    Calculate the Sharpe ratio for a series of returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : pd.Series\n",
    "        A Pandas Series containing the returns, typically in percentage terms.\n",
    "    annualize : bool, optional (default=True)\n",
    "        Whether to annualize the Sharpe ratio.\n",
    "    periods_per_year : int, optional (default=252)\n",
    "        The number of trading periods in a year. Defaults to 252 (daily returns).\n",
    "    risk_free_rate : float, optional (default=0.0)\n",
    "        The risk-free rate of return. Defaults to 0 for simplicity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The calculated Sharpe ratio. If annualized, returns the annualized Sharpe ratio.\n",
    "    \"\"\"\n",
    "    # Include only days where we actually traded:\n",
    "    returns = returns[returns != 0].copy()\n",
    "    \n",
    "    excess_returns = returns - risk_free_rate / periods_per_year\n",
    "\n",
    "    mean_return = excess_returns.mean()\n",
    "    std_return = excess_returns.std()\n",
    "\n",
    "    sharpe_ratio = mean_return / std_return\n",
    "\n",
    "    if annualize:\n",
    "        sharpe_ratio *= np.sqrt(periods_per_year)\n",
    "    \n",
    "    return sharpe_ratio\n",
    "\n",
    "def calculate_max_drawdown(net_capital):\n",
    "    \"\"\"\n",
    "    Calculate the maximum drawdown for a given series of net capital values.\n",
    "\n",
    "    Maximum drawdown is the largest peak-to-trough decline over a given time period,\n",
    "    representing the greatest loss from a high water mark to a subsequent low. This metric\n",
    "    is commonly used to assess the risk of a trading strategy by quantifying the worst\n",
    "    observed loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net_capital : pandas.Series\n",
    "        A time series of net capital values (e.g., account balance or portfolio value) \n",
    "        over a period of time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The maximum drawdown as a positive percentage, representing the largest relative \n",
    "        drop from a peak to a trough.\n",
    "    \"\"\"\n",
    "    drawdown = (net_capital - net_capital.cummax()) / net_capital.cummax() * 100\n",
    "    max_drawdown = drawdown.min()\n",
    "    return abs(max_drawdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Strategy Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align base strategy backtest's days with metalabeled strategy for comparison\n",
    "# Because we lost some days of evaluation on the metealabeled backtest due to the walk-forward approach\n",
    "base_strat_backtest_df = base_backtest_df.reindex(metalabeled_backtest_df.index)\n",
    "\n",
    "# Reset capital to provide an apples-to-apples comparison\n",
    "capital = 3000\n",
    "base_strat_backtest_df['net_capital'] = capital + (base_strat_backtest_df['net_pnl']*100).cumsum()\n",
    "base_strat_backtest_df['day_begin_net_capital'] = base_strat_backtest_df['net_capital'] - (base_strat_backtest_df['net_pnl']*100)\n",
    "base_strat_backtest_df['cumulative_pnl'] = base_strat_backtest_df['net_pnl'].cumsum()\n",
    "base_strat_backtest_df['pct_return'] = (base_strat_backtest_df['net_pnl']* 100)/ base_strat_backtest_df['day_begin_net_capital']\n",
    "\n",
    "px.line(base_strat_backtest_df['net_capital']).show()\n",
    "\n",
    "base_strat_sharpe = sharpe_ratio(returns=base_strat_backtest_df['pct_return'])\n",
    "base_strat_max_drawdown = calculate_max_drawdown(base_strat_backtest_df['net_capital'])\n",
    "base_strat_win_rate = len(base_strat_backtest_df[base_strat_backtest_df['net_pnl'] > 0]) / len(base_strat_backtest_df)\n",
    "base_strat_avg_win = base_strat_backtest_df[base_strat_backtest_df['net_pnl'] > 0]['net_pnl'].mean() * 100\n",
    "base_strat_avg_loss = abs(base_strat_backtest_df[base_strat_backtest_df['net_pnl'] < 0]['net_pnl'].mean() * 100)\n",
    "\n",
    "print(f\"Base strategy Sharpe: {round(base_strat_sharpe, 2)}\")\n",
    "print(f\"Base strategy maximum drawdown: {round(base_strat_max_drawdown, 2)}%\")\n",
    "print(f\"Base strategy win rate: {round(base_strat_win_rate * 100, 2)}%\")\n",
    "print(f\"Base strategy average win: ${round(base_strat_avg_win, 2)}\")\n",
    "print(f\"Base strategy average loss: ${round(base_strat_avg_loss, 2)}\")\n",
    "print(f\"Base strategy expected value per trade: ${round((base_strat_avg_win * base_strat_win_rate) - (base_strat_avg_loss * (1 - base_strat_win_rate)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-labeled Strategy Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the meta-model's trade/no trade reccomendations and add them to the base strategy backtest DataFrame to create a meta-labeled backtest DataFrame.\n",
    "# We will now only calculate PnL for a given day if the meta-model recommends to trade, setting net PnL to 0, effectively skipping the day otherwise\n",
    "metalabeled_strat_backtest_df = base_backtest_df.copy()\n",
    "aligned_preds = metalabeled_backtest_df['predicted_avoid_trade'].reindex(metalabeled_strat_backtest_df.index)\n",
    "metalabeled_strat_backtest_df['predicted_avoid_trade'] = aligned_preds\n",
    "metalabeled_strat_backtest_df = metalabeled_strat_backtest_df.dropna()\n",
    "metalabeled_strat_backtest_df['predicted_avoid_trade'] = metalabeled_strat_backtest_df['predicted_avoid_trade'].astype(int)\n",
    "\n",
    "metalabeled_strat_backtest_df['gross_pnl'] = metalabeled_strat_backtest_df.apply(calculate_pnl, axis=1)\n",
    "metalabeled_strat_backtest_df['net_pnl'] = np.where(metalabeled_strat_backtest_df['predicted_avoid_trade'] == 0, metalabeled_strat_backtest_df['gross_pnl'] * metalabeled_strat_backtest_df['contracts'] - metalabeled_strat_backtest_df['fees'], 0)\n",
    "\n",
    "capital = 3000\n",
    "\n",
    "metalabeled_strat_backtest_df['net_capital'] = capital + (metalabeled_strat_backtest_df['net_pnl']*100).cumsum()\n",
    "metalabeled_strat_backtest_df['day_begin_net_capital'] = metalabeled_strat_backtest_df['net_capital'] - (metalabeled_strat_backtest_df['net_pnl']*100)\n",
    "metalabeled_strat_backtest_df['cumulative_pnl'] = metalabeled_strat_backtest_df['net_pnl'].cumsum()\n",
    "metalabeled_strat_backtest_df['pct_return'] = (metalabeled_strat_backtest_df['net_pnl']* 100)/ metalabeled_strat_backtest_df['day_begin_net_capital']\n",
    "\n",
    "px.line(metalabeled_strat_backtest_df['net_capital']).show()\n",
    "\n",
    "metalabeled_strat_sharpe = sharpe_ratio(returns=metalabeled_strat_backtest_df['pct_return'])\n",
    "metalabeled_strat_max_drawdown = calculate_max_drawdown(metalabeled_strat_backtest_df['net_capital'])\n",
    "metalabeled_strat_win_rate = len(metalabeled_strat_backtest_df[metalabeled_strat_backtest_df['net_pnl'] > 0]) / len(metalabeled_strat_backtest_df[metalabeled_strat_backtest_df['predicted_avoid_trade'] == 0])\n",
    "metalabeled_strat_avg_win = metalabeled_strat_backtest_df[metalabeled_strat_backtest_df['net_pnl'] > 0]['net_pnl'].mean() * 100\n",
    "metalabeled_strat_avg_loss = abs(metalabeled_strat_backtest_df[metalabeled_strat_backtest_df['net_pnl'] < 0]['net_pnl'].mean() * 100)\n",
    "\n",
    "print(f\"Meta-labeled strategy Sharpe: {round(metalabeled_strat_sharpe, 2)}\")\n",
    "print(f\"Meta-labeled strategy maximum drawdown: {round(metalabeled_strat_max_drawdown, 2)}%\")\n",
    "print(f\"Meta-labeled strategy win rate: {round(metalabeled_strat_win_rate * 100, 2)}%\")\n",
    "print(f\"Meta-labeled strategy average win: ${round(metalabeled_strat_avg_win, 2)}\")\n",
    "print(f\"Meta-labeled strategy average loss: ${round(metalabeled_strat_avg_loss, 2)}\")\n",
    "print(f\"Meta-labeled strategy expected value per trade: ${round((metalabeled_strat_avg_win * metalabeled_strat_win_rate) - (metalabeled_strat_avg_loss * (1 - metalabeled_strat_win_rate)), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_improvement = (sharpe_ratio(returns=metalabeled_strat_backtest_df['pct_return']) - sharpe_ratio(returns=base_strat_backtest_df['pct_return'])) / sharpe_ratio(returns=base_strat_backtest_df['pct_return'])\n",
    "max_drawdown_improvement = metalabeled_strat_max_drawdown - base_strat_max_drawdown\n",
    "avg_win_improvement = (metalabeled_strat_avg_win - base_strat_avg_win) / base_strat_avg_win\n",
    "avg_loss_improvement = (metalabeled_strat_avg_loss - base_strat_avg_loss) / base_strat_avg_loss\n",
    "ev_improvement = (((metalabeled_strat_avg_win * metalabeled_strat_win_rate) - (metalabeled_strat_avg_loss * (1 - metalabeled_strat_win_rate))) - ((base_strat_avg_win * base_strat_win_rate) - (base_strat_avg_loss * (1 - base_strat_win_rate)))) / ((base_strat_avg_win * base_strat_win_rate) - (base_strat_avg_loss * (1 - base_strat_win_rate)))\n",
    "\n",
    "print(f\"Sharpe Improvement: {round(sharpe_improvement * 100, 2)}%\")\n",
    "print(f\"Maxmimum Drawdown Improvement: {round(max_drawdown_improvement, 2)}%\")\n",
    "print(f\"Average Win Improvement: {round(avg_win_improvement * 100, 2)}%\")\n",
    "print(f\"Average Loss Improvement: {round(avg_loss_improvement * 100, 2)}%\")\n",
    "print(f\"EV Per Trade Improvement: {round(ev_improvement * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realized Vs. Implied Volatility Plots and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implied and realized volatility metrics\n",
    "base_strat_backtest_df['trade_to_close_vol'] = abs((base_strat_backtest_df['underlying_closing_price'] - base_strat_backtest_df['underlying_price_at_trade']) / base_strat_backtest_df['underlying_price_at_trade']) * 100\n",
    "base_strat_backtest_df['current_day_IV'] = base_strat_backtest_df['vix1d_value'] / np.sqrt(252)\n",
    "base_strat_backtest_df['current_day_VRP'] = base_strat_backtest_df['current_day_IV'] - base_strat_backtest_df['trade_to_close_vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=(\"Daily VRPs Scatter Plot\", \"Histogram of Current Day VRP\"),\n",
    "    row_heights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Scatter Plot in the First Row\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=base_strat_backtest_df.index,  # x-axis as index (or replace with desired x-axis data)\n",
    "        y=base_strat_backtest_df['current_day_VRP'],\n",
    "        mode='markers',\n",
    "        marker=dict(color='blue'),\n",
    "        name='Current Day VRP'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add a red line at y = 0 in the scatter plot\n",
    "fig.add_hline(\n",
    "    y=0, line=dict(color='red', width=2), row=1, col=1\n",
    ")\n",
    "\n",
    "# Histogram in the Second Row with lighter blue, translucent bars, and defined borders\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=base_strat_backtest_df['current_day_VRP'],\n",
    "        marker=dict(color='lightblue', opacity=0.6, line=dict(color='blue', width=1)),\n",
    "        name='VRP Distribution'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Update layout with increased height and hide legend\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"VRP\",\n",
    "    xaxis2_title=\"VRP\",\n",
    "    yaxis2_title=\"Frequency\",\n",
    "    height=800,  # Increase this value to make the figure taller\n",
    "    showlegend=False  # Hide the legend\n",
    ")\n",
    "\n",
    "# Show combined plot\n",
    "fig.show()\n",
    "\n",
    "overestimation_pct = len(base_strat_backtest_df[base_strat_backtest_df['current_day_VRP'] > 0]) / len(base_strat_backtest_df)\n",
    "mean_overestimation_when_overestimates = base_strat_backtest_df[base_strat_backtest_df['current_day_VRP'] > 0]['current_day_VRP'].mean()\n",
    "mean_overestimation_total = base_strat_backtest_df['current_day_VRP'].mean()\n",
    "\n",
    "print(f\"Overestimation %: {round(overestimation_pct * 100, 2)}%\")\n",
    "print(f\"Mean overestimation when VIX1D overestimates: {round(mean_overestimation_when_overestimates, 2)}%\")\n",
    "print(f\"Mean overestimation: {round(mean_overestimation_total, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
