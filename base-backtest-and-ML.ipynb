{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from pandas_market_calendars import get_calendar\n",
    "import requests\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sqlalchemy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data + add relevant info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('mysql+mysqlconnector://Teymour:Teymour1289@teymourdb.c9co4oue0tpq.us-east-2.rds.amazonaws.com:3306/quant_data')\n",
    "query = \"SELECT * FROM sp500_vol_sell_backtest_dataset\"\n",
    "\n",
    "base_backtest_df = pd.read_sql(query, engine)\n",
    "base_backtest_df = data.set_index('date')\n",
    "base_backtest_df.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading assumptions and max loss calculations\n",
    "base_backtest_df['nat_price_cost'] = base_backtest_df['short_bid_price'] - base_backtest_df['long_ask_price']\n",
    "base_backtest_df['max_nat_price_loss'] = abs(base_backtest_df['short_strike'].iloc[0] - base_backtest_df['long_strike'].iloc[0]) - base_backtest_df['nat_price_cost']\n",
    "base_backtest_df['mid_price_cost'] = base_backtest_df['short_mid_price'] - base_backtest_df['long_mid_price']\n",
    "base_backtest_df['max_mid_price_loss'] = abs(base_backtest_df['short_strike'].iloc[0] - base_backtest_df['long_strike'].iloc[0]) - base_backtest_df['mid_price_cost']\n",
    "base_backtest_df[\"contracts\"] = 1\n",
    "base_backtest_df[\"fees\"] = base_backtest_df[\"contracts\"] * 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implied and realized volatility metrics\n",
    "base_backtest_df['trade_to_close_vol'] = abs((base_backtest_df['underlying_price_at_trade'] - base_backtest_df['underlying_closing_price']) / base_backtest_df['underlying_price_at_trade']) * 100\n",
    "base_backtest_df['current_day_IV'] = base_backtest_df['vix1d_value'] / np.sqrt(252)\n",
    "base_backtest_df['current_day_VRP'] = base_backtest_df['current_day_IV'] - base_backtest_df['trade_to_close_vol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pnl(row):\n",
    "    if row['direction'] == 1:\n",
    "        settlement = row['underlying_closing_price'] - row['short_strike']\n",
    "        if settlement > 0:\n",
    "            settlement = 0\n",
    "            final_pnl = row['mid_price_cost']\n",
    "        else:\n",
    "            final_pnl = settlement + row['mid_price_cost']\n",
    "            \n",
    "    elif row['direction'] == 0:\n",
    "        settlement = row['short_strike'] - row['underlying_closing_price']\n",
    "        if settlement > 0:\n",
    "            settlement = 0\n",
    "            final_pnl = row['mid_price_cost']\n",
    "        else:\n",
    "            final_pnl = settlement + row['mid_price_cost']\n",
    "\n",
    "    gross_pnl = np.maximum(final_pnl, row['max_mid_price_loss'] * -1)\n",
    "    \n",
    "    return gross_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_backtest_df['gross_pnl'] = base_backtest_df.apply(calculate_pnl, axis=1)\n",
    "base_backtest_df['net_pnl'] = (base_backtest_df['gross_pnl'] * base_backtest_df['contracts']) - base_backtest_df['fees']\n",
    "\n",
    "capital = 100\n",
    "\n",
    "base_backtest_df['net_capital'] = capital + (base_backtest_df['net_pnl']*100).cumsum()\n",
    "base_backtest_df['cumulative_pnl'] = base_backtest_df['net_pnl'].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM sp500_daily_OHLCV\"\n",
    "ml_data = pd.read_sql(query, engine)\n",
    "ml_data.set_index('t', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features from underlying OHLC data\n",
    "for days in range(1, 6):\n",
    "    ml_data[f'return_{days}d'] = ml_data['c'].pct_change(periods=days)\n",
    "\n",
    "for lag in range(1, 6):\n",
    "    ml_data[f'lag_{lag}d'] = ml_data['c'].shift(lag)\n",
    "\n",
    "for lag in range(3, 6):\n",
    "    ml_data[f'serial_corr_{lag}d'] = ml_data['c'].rolling(window=lag).apply(lambda x: x.autocorr(), raw=False)\n",
    "\n",
    "ml_data['50d_volatility'] = ml_data['c'].rolling(window=50).std()\n",
    "\n",
    "vol_windows = [10, 20, 50, 100]\n",
    "for window in vol_windows:\n",
    "    ml_data[f'{window}d_volatility'] = ml_data['c'].rolling(window=window).std()\n",
    "\n",
    "ml_data['high_low_range'] = ml_data['h'] - ml_data['l']\n",
    "for window in vol_windows:\n",
    "    ml_data[f'{window}d_high_low_vol'] = ml_data['high_low_range'].rolling(window=window).std()\n",
    "\n",
    "ml_data['tr'] = np.maximum((ml_data['h'] - ml_data['l']), \n",
    "                           np.maximum(abs(ml_data['h'] - ml_data['c'].shift(1)),\n",
    "                                      abs(ml_data['l'] - ml_data['c'].shift(1))))\n",
    "ml_data['14d_ATR'] = ml_data['tr'].rolling(window=14).mean()\n",
    "\n",
    "\n",
    "aligned_vrp = base_backtest_df['current_day_VRP'].reindex(ml_data.index)\n",
    "ml_data = pd.concat([ml_data, aligned_vrp], axis=1)\n",
    "\n",
    "def compute_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "ml_data['14d_RSI'] = compute_rsi(ml_data['c'], window=14)\n",
    "\n",
    "ma_windows = [10, 50, 100, 200]\n",
    "for window in ma_windows:\n",
    "    ml_data[f'{window}d_MA'] = ml_data['c'].rolling(window=window).mean()\n",
    "\n",
    "ml_data['12d_EMA'] = ml_data['c'].ewm(span=12, adjust=False).mean()\n",
    "ml_data['26d_EMA'] = ml_data['c'].ewm(span=26, adjust=False).mean()\n",
    "ml_data['MACD'] = ml_data['12d_EMA'] - ml_data['26d_EMA']\n",
    "ml_data['MACD_signal'] = ml_data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "ml_data['momentum_5d'] = ml_data['c'] / ml_data['c'].shift(5) - 1\n",
    "ml_data['momentum_10d'] = ml_data['c'] / ml_data['c'].shift(10) - 1\n",
    "\n",
    "ml_data['20d_MA'] = ml_data['c'].rolling(window=20).mean()\n",
    "ml_data['20d_stddev'] = ml_data['c'].rolling(window=20).std()\n",
    "ml_data['upper_band'] = ml_data['20d_MA'] + (ml_data['20d_stddev'] * 2)\n",
    "ml_data['lower_band'] = ml_data['20d_MA'] - (ml_data['20d_stddev'] * 2)\n",
    "\n",
    "ml_data['volatility_ratio'] = ml_data['10d_volatility'] / ml_data['50d_volatility']\n",
    "\n",
    "ml_data['20d_high'] = ml_data['h'].rolling(window=20).max()\n",
    "ml_data['20d_low'] = ml_data['l'].rolling(window=20).min()\n",
    "\n",
    "ml_data['14d_ATRP'] = ml_data['14d_ATR'] / ml_data['c'] * 100\n",
    "\n",
    "ml_data['MA_crossover_10_50'] = np.where(ml_data['10d_MA'] > ml_data['50d_MA'], 1, 0)\n",
    "\n",
    "ml_data['price_to_50d_MA'] = ml_data['c'] / ml_data['50d_MA']\n",
    "ml_data['price_to_200d_MA'] = ml_data['c'] / ml_data['200d_MA']\n",
    "\n",
    "# Final cleanup: Drop intermediate calculation columns that were temporary\n",
    "ml_data.drop(['tr', '20d_stddev', '20d_MA'], axis=1, inplace=True)\n",
    "\n",
    "ml_data = ml_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable - whether we should have traded on a given day or not\n",
    "base_backtest_df['to_trade'] = np.where(base_backtest_df['net_pnl'] >= 0, 1, 0 )\n",
    "aligned_target = base_backtest_df['to_trade'].reindex(ml_data.index)\n",
    "\n",
    "ml_data = pd.concat([ml_data, aligned_target], axis=1)\n",
    "ml_data = ml_data.rename(columns={'to_trade': 'target'})\n",
    "\n",
    "X = ml_data.drop(['o', 'c', 'h', 'l', 'target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metalabel(data, training_periods, testing_periods, quant_feature_list, cat_feature_list):\n",
    "    data = data[:-1].copy()\n",
    "    \n",
    "    keys, backtest_keys, period_data_dict, backtest_period_data_dict = fr.group_by_period(\n",
    "        data, training_periods, testing_periods\n",
    "    )\n",
    "\n",
    "    agg_backtest_df = pd.DataFrame()\n",
    "    num_iterations = len(keys)\n",
    "\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        model_key = keys[i]\n",
    "        train_df = period_data_dict[model_key].copy()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        train_df[quant_feature_list] = scaler.fit_transform(train_df[quant_feature_list])\n",
    "\n",
    "        all_features = quant_feature_list + cat_feature_list\n",
    "        \n",
    "        train_features = train_df[all_features]\n",
    "        train_target = train_df['target'].values.flatten()\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "                                   loss_function='Logloss',\n",
    "                                   eval_metric='Logloss',\n",
    "                                   task_type='CPU',\n",
    "                                   cat_features=[f for f in all_features if f in cat_feature_list],\n",
    "                                   verbose=False)\n",
    "\n",
    "        model.fit(train_features, train_target,\n",
    "                  early_stopping_rounds=20,\n",
    "                  plot=False)\n",
    "\n",
    "        backtest_key = backtest_keys[i]\n",
    "        backtest_df = backtest_period_data_dict[backtest_key].copy()\n",
    "        \n",
    "        backtest_df[quant_feature_list] = scaler.transform(backtest_df[quant_feature_list])\n",
    "        \n",
    "        test_features = backtest_df[all_features]\n",
    "\n",
    "        probabilities = model.predict_proba(test_features)[:, 1] \n",
    "        predictions = (probabilities > 0.5).astype(int) \n",
    "        confidence = np.maximum(probabilities, 1 - probabilities)  \n",
    "\n",
    "        prediction_df = pd.DataFrame({\n",
    "            'predicted_trade_action': predictions,\n",
    "            'prediction_confidence': confidence\n",
    "        }, index=backtest_df.index)\n",
    "\n",
    "        backtest_df = backtest_df.join(prediction_df)\n",
    "\n",
    "        agg_backtest_df = pd.concat([agg_backtest_df, backtest_df], axis=0)\n",
    "\n",
    "    return agg_backtest_df\n",
    "\n",
    "metalabeled_backtest_df = metalabel(data=ml_data, training_periods=150, testing_periods=1, quant_feature_list=list(X.columns), cat_feature_list=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = backtest_df['target']\n",
    "y_pred = backtest_df['predicted_trade_action']\n",
    "\n",
    "# Calculate the classification metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Detailed classification report\n",
    "report = classification_report(y_true, y_pred, target_names=['No Trade', 'Trade'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalabeled_data = data.copy()\n",
    "aligned_preds = backtest_df['predicted_trade_action'].reindex(metalabeled_data.index)\n",
    "metalabeled_data['predicted_trade_action'] = aligned_preds\n",
    "metalabeled_data = metalabeled_data.dropna()\n",
    "metalabeled_data['predicted_trade_action'] = metalabeled_data['predicted_trade_action'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalabeled_data['gross_pnl'] = metalabeled_data.apply(calculate_pnl, axis=1)\n",
    "metalabeled_data['net_pnl'] = np.where(metalabeled_data['predicted_trade_action'] == 1, metalabeled_data['gross_pnl'] * metalabeled_data['contracts'] - metalabeled_data['fees'], 0)\n",
    "\n",
    "capital = 3000\n",
    "\n",
    "metalabeled_data['net_capital'] = capital + (metalabeled_data['net_pnl']*100).cumsum()\n",
    "metalabeled_data['cumulative_pnl'] = metalabeled_data['net_pnl'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(metalabeled_data['net_capital'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(data['net_capital'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = backtest_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
